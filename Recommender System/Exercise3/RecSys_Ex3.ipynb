{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THQFNe3zdt1f"
   },
   "source": [
    "# RecSys M 2021 - Exercise 3 Template\n",
    "\n",
    "The aims of this exercise are:\n",
    " - Explore a different recommendation dataset\n",
    " - Develop and evaluate baseline recommender systems\n",
    " - Implement hybrid recommender models\n",
    " - Explore diversification issues in recommender systems\n",
    " - Revise other material from the lectures.\n",
    "\n",
    "As usual, there is a corresponding Quiz on Moodle for this Exercise, which should be answered as you proceed. For more details, see the Exercise 3 specification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvTdMyXdODex"
   },
   "source": [
    "# Part-Pre. Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww--_kl9-ndn"
   },
   "source": [
    "## Pre 1. Setup Block\n",
    "\n",
    "This exercise will use the [Goodreads]() dataset for books. These blocks setup the data files, Python etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:39:07.562968Z",
     "start_time": "2021-08-06T12:39:07.560713Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFgYpbhh0tkX",
    "outputId": "ba649093-2d69-4bdc-f095-4eba4952fb01"
   },
   "outputs": [],
   "source": [
    "# !rm -rf ratings* books* to_read* test*\n",
    "\n",
    "# !curl -o ratings.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-ratings.csv\" \n",
    "# !curl -o books.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-books.csv\"\n",
    "# !curl -o to_read.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-to_read.csv\"\n",
    "# !curl -o test.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:22:30.024292Z",
     "start_time": "2021-08-19T09:22:27.676326Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VpVnNrZ1EiX",
    "outputId": "0c078e38-ce42-45d2-8295-4f0cee2c305d"
   },
   "outputs": [],
   "source": [
    "#Standard setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# !pip install git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight\n",
    "from spotlight.interactions import Interactions\n",
    "SEED=20\n",
    "BPRMF=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtJO0e0m-hun"
   },
   "source": [
    "## Pre 2. Data Preparation\n",
    "\n",
    "Let's load the dataset into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:22:34.627319Z",
     "start_time": "2021-08-19T09:22:34.306169Z"
    },
    "id": "qKAb25iw1MYw"
   },
   "outputs": [],
   "source": [
    "#load in the csv files\n",
    "ratings_df = pd.read_csv(\"ratings.csv\")\n",
    "books_df = pd.read_csv(\"books.csv\")\n",
    "to_read_df = pd.read_csv(\"to_read.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:22:35.898758Z",
     "start_time": "2021-08-19T09:22:35.392423Z"
    },
    "id": "W6rqfn53OhDC"
   },
   "outputs": [],
   "source": [
    "#cut down the number of items and users\n",
    "counts=ratings_df[ratings_df[\"book_id\"] < 2000].groupby([\"book_id\"]).count().reset_index()\n",
    "valid_books=counts[counts[\"user_id\"] >= 10][[\"book_id\"]]\n",
    "\n",
    "books_df = books_df.merge(valid_books, on=\"book_id\")\n",
    "ratings_df = ratings_df[ratings_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "to_read_df = to_read_df[to_read_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "test = test[test[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "\n",
    "\n",
    "#stringify the id columns\n",
    "def str_col(df):\n",
    "  if \"user_id\" in df.columns:\n",
    "    df[\"user_id\"] = \"u\" + df.user_id.astype(str)\n",
    "  if \"book_id\" in df.columns:\n",
    "    df[\"book_id\"] = \"b\" + df.book_id.astype(str)\n",
    "\n",
    "str_col(books_df)\n",
    "str_col(ratings_df)\n",
    "str_col(to_read_df)\n",
    "str_col(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7cgXhmYUXIn"
   },
   "source": [
    "Here we construct the Interactions objects from `ratings.csv`, `to_read.csv` and `test.csv`. We manually specify the num_users and num_items parameters to all Interactions objects, in case the test set differs from your training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:22:42.130260Z",
     "start_time": "2021-08-19T09:22:40.572784Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15ClgJOdTTt1",
    "outputId": "7ad66b8a-d396-4827-9e00-46fbd7632a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interactions dataset (1999 users x 1826 items x 124762 interactions)>\n",
      "<Interactions dataset (1999 users x 1826 items x 135615 interactions)>\n",
      "<Interactions dataset (1999 users x 1826 items x 33917 interactions)>\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "\n",
    "iid_map = defaultdict(count().__next__)\n",
    "\n",
    "\n",
    "rating_iids = np.array([iid_map[iid] for iid in ratings_df[\"book_id\"].values], dtype = np.int32)\n",
    "test_iids = np.array([iid_map[iid] for iid in test[\"book_id\"].values], dtype = np.int32)\n",
    "toread_iids = np.array([iid_map[iid] for iid in to_read_df[\"book_id\"].values], dtype = np.int32)\n",
    "\n",
    "\n",
    "uid_map = defaultdict(count().__next__)\n",
    "test_uids = np.array([uid_map[uid] for uid in test[\"user_id\"].values], dtype = np.int32)\n",
    "rating_uids = np.array([uid_map[uid] for uid in ratings_df[\"user_id\"].values], dtype = np.int32)\n",
    "toread_uids = np.array([uid_map[iid] for iid in to_read_df[\"user_id\"].values], dtype = np.int32)\n",
    "\n",
    "\n",
    "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
    "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
    "\n",
    "\n",
    "rating_dataset = Interactions(user_ids=rating_uids,\n",
    "                               item_ids=rating_iids,\n",
    "                               ratings=ratings_df[\"rating\"].values,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "toread_dataset = Interactions(user_ids=toread_uids,\n",
    "                               item_ids=toread_iids,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "test_dataset = Interactions(user_ids=test_uids,\n",
    "                               item_ids=test_iids,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "print(rating_dataset)\n",
    "print(toread_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "#here we define the validation set\n",
    "toread_dataset_train, validation = random_train_test_split(toread_dataset, random_state=np.random.RandomState(SEED))\n",
    "\n",
    "num_items = test_dataset.num_items\n",
    "num_users = test_dataset.num_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2noK30pBEsF"
   },
   "source": [
    "Finally, this is some utility code that we will use in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:22:42.301461Z",
     "start_time": "2021-08-19T09:22:42.292214Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kDxZgICBFp6",
    "outputId": "571cf91a-c0e8-4995-a36b-d7f66205d9b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid 0: Carlos Ruiz Zafón, Lucia Graves / The Shadow of the Wind (The Cemetery of Forgotten Books,  #1)\n"
     ]
    }
   ],
   "source": [
    "def getAuthorTitle(iid):\n",
    "  bookid = iid_rev_map[iid]\n",
    "  row = books_df[books_df.book_id == bookid]\n",
    "  return row.iloc[0][\"authors\"] + \" / \" + row.iloc[0][\"title\"]\n",
    "\n",
    "print(\"iid 0: \" + getAuthorTitle(0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt4I2C5DTUL5"
   },
   "source": [
    "## Pre 3. Example Code\n",
    "\n",
    "To evaluate some of your hand-implemented recommender systems (e.g. Q1, Q4), you will need to instantiate objects that match the specification of a Spotlight model, which `mrr_score()` etc. expects.\n",
    "\n",
    "\n",
    "Here is an example recommender object that returns 0 for each item, regardless of user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:22:47.194697Z",
     "start_time": "2021-08-19T09:22:46.715410Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2eaxy_hakbC",
    "outputId": "292715a8-c838-4e44-bd44-47f52d451361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from spotlight.evaluation import mrr_score, precision_recall_score\n",
    "\n",
    "class dummymodel:\n",
    "  \n",
    "  def __init__(self, numitems):\n",
    "    self.predictions=np.zeros(numitems)\n",
    "  \n",
    "  #uid is the user we are requesting recommendations for;\n",
    "  #returns an array of scores, one for each item\n",
    "  def predict(self, uid):\n",
    "    #this model returns all zeros, regardless of userid\n",
    "    return( self.predictions )\n",
    "\n",
    "#lets evaluate how the effeciveness of dummymodel\n",
    "\n",
    "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100).mean())\n",
    "#as expected, a recommendation model that gives 0 scores for all items obtains a MRR score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:22:48.670747Z",
     "start_time": "2021-08-19T09:22:48.138967Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQTJOmS5dB3i",
    "outputId": "392bd5b6-3fc1-435c-f872-9262596f22d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [00:00, 4004.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#note that mrr_score() displays a progress bar if you set verbose=True\n",
    "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCWXwVC5Mtyj"
   },
   "source": [
    "# Part-A. Combination of Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyvGgW_3ZjLV"
   },
   "source": [
    "## Task 1. Explicit & Implicit Matrix Factorisation Models\n",
    "\n",
    "Create and train three matrix factorisation systems:\n",
    " - \"EMF\": explicit MF, trained on the ratings Interactions object (`rating_dataset`)\n",
    " - \"IMF\": implicit MF, trained on the toread_dataset Interactions object (`toread_dataset_train`)\n",
    " - \"BPRMF\": implicit MF with the BPR loss function (`loss='bpr'`), trained on the toread_dataset Interactions object (`toread_dataset_train`)\n",
    "\n",
    "Use a variable of the same name for these models, as we will use some of them later (e.g. `BPRMF`).\n",
    "  \n",
    "In all cases, you must use the standard initialisation arguments, i.e. \n",
    "`n_iter=10, embedding_dim=32, use_cuda=False, random_state=np.random.RandomState(SEED)`.\n",
    " \n",
    "Evaluate each of these models in terms of Mean Reciprocal Rank on the test set. MRR can be obtained using:\n",
    "```python\n",
    "mrr_score(X, test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n",
    "```\n",
    "where X is an instance of a Spotlight model. Do NOT change the `k` or `train` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:24:15.096478Z",
     "start_time": "2021-08-19T09:22:55.663465Z"
    },
    "id": "qDADjtepRvpJ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "109it [00:00, 1084.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 17 seconds for EMF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [00:01, 1195.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR for EMF: 0.05898399982013507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:00, 1114.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 28 seconds for IMF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [00:01, 1239.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR for IMF: 0.3299315791285401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [00:00, 1000.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 28 seconds for BPRMF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [00:01, 1126.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR for BPRMF: 0.4076771464674879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add your solution here\n",
    "import time\n",
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "\n",
    "# explicit MF, trained on the ratings Interactions object (rating_dataset)\n",
    "EMF = ExplicitFactorizationModel(n_iter=10,\n",
    "                                 embedding_dim=32,\n",
    "                                 use_cuda=False,\n",
    "                                 random_state=np.random.RandomState(SEED)) #20\n",
    "current = time.time()\n",
    "EMF.fit(rating_dataset)\n",
    "end = time.time()\n",
    "print(\"Training took %d seconds for EMF\"% ((end - current)))\n",
    "EMF_rr = mrr_score(EMF, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
    "print(\"MRR for EMF:\", EMF_rr.mean())\n",
    "\n",
    "# implicit MF, trained on the toread_dataset Interactions object (toread_dataset_train)\n",
    "IMF = ImplicitFactorizationModel(n_iter=10, \n",
    "                                 embedding_dim=32,\n",
    "                                 use_cuda=False,\n",
    "                                 random_state=np.random.RandomState(SEED))\n",
    "current = time.time()\n",
    "IMF.fit(toread_dataset_train)\n",
    "end = time.time()\n",
    "print(\"Training took %d seconds for IMF\"% ((end - current)))\n",
    "IMF_rr = mrr_score(IMF, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
    "print(\"MRR for IMF:\", IMF_rr.mean())\n",
    "\n",
    "# implicit MF with the BPR loss function (loss='bpr'), trained on the toread_dataset Interactions object (toread_dataset_train)\n",
    "BPRMF = ImplicitFactorizationModel(n_iter=10, \n",
    "                                   embedding_dim=32,\n",
    "                                   use_cuda=False,\n",
    "                                   random_state=np.random.RandomState(SEED),\n",
    "                                   loss='bpr')\n",
    "current = time.time()\n",
    "BPRMF.fit(toread_dataset_train)\n",
    "end = time.time()\n",
    "print(\"Training took %d seconds for BPRMF\"% ((end - current)))\n",
    "BPR_rr = mrr_score(BPRMF, test_dataset, train=rating_dataset, k=100, verbose=True)\n",
    "print(\"MRR for BPRMF:\", BPR_rr.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZHCOmfEDOGo"
   },
   "source": [
    "## Task 2. Hybrid Model\n",
    "\n",
    "In this task, you are expected to create new hybrid recommendation models that \n",
    "combine the two models in Task 1, namely IMF and BPRMF. \n",
    "\n",
    "(a) Linearly combine the *scores* from IMF and BPRMF.  Normalise both input scores into the range 0..1 using [sklearn's minmax_scale() function](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) before combining them.\n",
    "\n",
    "(b) Apply a pipelining recommender, where the top 100 items are obtained from IMF and re-ranked using the scores of BPRMF. Items not returned by IMF get a score of 0.\n",
    "\n",
    "To implement these hybrid models, you should create new classes that abide by the Spotlight model contract (namely, it has a `predict(self, uid)` function that returns a score for *all* items). \n",
    "\n",
    "Evaluate each model in terms of MRR. How many users are improved, how many are degraded compared to the BPRMF baseline?\n",
    "\n",
    "Finally, pass your instantiated model object to the `test_Hybrid_a()` (for (a)) or `test_Hybrid_b()` (for (b)) functions, as appropriate, and record the results in the quiz. For example, if your model for (b) is called `pipeline`, then you would run:\n",
    "```python\n",
    "test_Hybrid_b(pipeline)\n",
    "```\n",
    "\n",
    "You now have sufficient information to answer the Task 2 quiz questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:26.877816Z",
     "start_time": "2021-08-06T12:40:26.873242Z"
    },
    "id": "6j6JzIOHkYw9"
   },
   "outputs": [],
   "source": [
    "def test_Hybrid_a(combsumObj):\n",
    "  for i, u in enumerate([5, 20]):\n",
    "    print(\"Hybrid a test case %d\" % i)\n",
    "    print(np.count_nonzero(combsumObj.predict(u) > 1))\n",
    "\n",
    "def test_Hybrid_b(pipeObj):\n",
    "  for i, iid in enumerate([3, 0]):\n",
    "    print(\"Hybrid b test case %d\" % i)\n",
    "    print(pipeObj.predict(0)[iid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:28.846772Z",
     "start_time": "2021-08-06T12:40:26.880472Z"
    },
    "id": "1_o7a1ppFZ7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2617672345739065\n"
     ]
    }
   ],
   "source": [
    "# Add your solutions here and evaluate them\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# for hybrid models\n",
    "class hybridModel:\n",
    "    def __init__(self, scores):\n",
    "        self.predictions=scores\n",
    "# uid is the user we are requesting recommendations for;\n",
    "# returns an array of scores, one for each item\n",
    "    def predict(self, uid):\n",
    "    #this model returns a score of all items for each user\n",
    "        return self.predictions[uid]\n",
    "\n",
    "# scores for linear model\n",
    "# normalised IMF and BPR scores\n",
    "normal_IMF = minmax_scale([IMF.predict(i) for i in range(num_users)])\n",
    "normal_BPR = minmax_scale([BPRMF.predict(i) for i in range(num_users)])\n",
    "# linearly combine them\n",
    "linear_model = hybridModel(normal_IMF + normal_BPR)\n",
    "# evaluate how the effectiveness of linear model\n",
    "linear_rr = mrr_score(linear_model, test_dataset, train=rating_dataset, k=100)\n",
    "print(linear_rr.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:28.863073Z",
     "start_time": "2021-08-06T12:40:28.848415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582\n",
      "1227\n"
     ]
    }
   ],
   "source": [
    "# improved compared with BPR\n",
    "print(sum(item > 0 for item in (linear_rr - BPR_rr)))\n",
    "# degraded by linear\n",
    "print(sum(item < 0 for item in (linear_rr - BPR_rr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:32.512383Z",
     "start_time": "2021-08-06T12:40:28.864766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41511955311399246\n"
     ]
    }
   ],
   "source": [
    "# scores for pipeline recommender\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "pipe_score = []\n",
    "for uid in range(num_users):\n",
    "#     IMF and BPR score for a single user\n",
    "    IMF_score = IMF.predict(uid)\n",
    "    BPR_score = BPRMF.predict(uid)\n",
    "#     rank IMF score to get top 100 items\n",
    "    ranks = rankdata(IMF_score)\n",
    "#     score = BPR_score if this item ranks top 100 otherwise score = 0\n",
    "    rerank_score = [BPR_score[idx] if ranks[idx] > (num_items - 100) else 0 for idx in range(num_items)]\n",
    "    pipe_score.append(rerank_score)\n",
    "    \n",
    "# pipeline model\n",
    "pipe_model = hybridModel(np.array(pipe_score))\n",
    "# evaluate how the effectiveness of pipeline\n",
    "pipe_rr = mrr_score(pipe_model, test_dataset, train=rating_dataset, k=100)\n",
    "print(pipe_rr.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:32.529387Z",
     "start_time": "2021-08-06T12:40:32.514611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "# improved compared with BPR\n",
    "print(sum(item > 0 for item in (pipe_rr - BPR_rr)))\n",
    "# degraded by pipeline\n",
    "print(sum(item < 0 for item in (pipe_rr - BPR_rr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:32.534512Z",
     "start_time": "2021-08-06T12:40:32.531200Z"
    },
    "id": "ROCBxlfKKkGA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid a test case 0\n",
      "800\n",
      "Hybrid a test case 1\n",
      "658\n",
      "Hybrid b test case 0\n",
      "22.01358413696289\n",
      "Hybrid b test case 1\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Now test your hybrid approaches for the quiz\n",
    "\n",
    "test_Hybrid_a(linear_model)\n",
    "test_Hybrid_b(pipe_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf0K6GECM0LQ"
   },
   "source": [
    "# Part-B. Analysing Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gszqk2kIZLX"
   },
   "source": [
    "## Utility methods\n",
    "\n",
    "Below, we provide a function, `get_top_K(model, uid : int, k : int)` which, when provided with a Spotlight model, will provide the top k predictions for the specified uid. The iids, their scores, and their embeddings are returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:32.555837Z",
     "start_time": "2021-08-06T12:40:32.537396Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIyc_p_dIm1M",
    "outputId": "13c5f9f3-bae3-4665-8486-014e7e21c7e7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned iids: [ 23 108  21  33   9  81  52 254  16   3]\n",
      "Returned scores: [1.         0.98951113 0.9848312  0.92250615 0.9070821  0.9065416\n",
      " 0.90052915 0.8930985  0.88378024 0.8836938 ]\n",
      "Returned embeddings: tensor([[-0.0453,  1.3716, -0.8307, -1.2615,  1.6700,  1.0161,  1.1169,  2.3530,\n",
      "         -1.2027,  0.8522, -1.0941, -0.6865, -0.5726, -2.0335, -1.2590,  0.6154,\n",
      "         -0.1374, -1.6869, -1.8616, -0.7514,  1.9909, -0.3910,  1.9240,  1.3294,\n",
      "         -1.2834, -0.4520,  1.1338,  0.3468,  2.5168, -2.1587,  1.2310,  1.1670],\n",
      "        [ 0.1240,  1.1004,  0.0531, -1.1045,  1.9932,  1.5049,  1.0011,  1.9734,\n",
      "         -1.6322, -0.8913, -0.6372,  0.7721, -1.1422, -2.2424, -1.1936, -0.5770,\n",
      "          0.0762, -1.0283, -1.2807, -2.0889,  2.8154, -0.9600, -0.1419,  0.8408,\n",
      "         -1.6067, -1.2906,  1.9169,  1.3988,  1.8646, -2.2029,  0.5365,  0.2022],\n",
      "        [ 0.3844,  0.8188, -0.1892, -1.1793,  2.1731,  0.6669,  1.1271,  1.4538,\n",
      "         -1.2173, -0.5447, -1.6714,  0.5249, -0.6132, -3.1083, -0.6489,  0.4312,\n",
      "          0.9176, -1.0346, -1.7232, -1.3347,  2.5504,  0.2789,  1.9649,  0.7684,\n",
      "         -1.0310, -1.3983,  0.8985, -0.0561,  2.1894, -0.8905,  1.0991,  0.6691],\n",
      "        [-0.4411,  0.4801, -0.2539, -0.5986,  1.2272,  0.6531,  1.4534,  1.3803,\n",
      "         -1.3797,  0.8306, -1.1837, -0.3366, -0.3527, -1.9982, -1.2019,  0.8934,\n",
      "         -0.5632, -0.6443, -0.7336, -0.4921,  2.9899,  0.2760,  1.4479,  1.0105,\n",
      "         -0.7107, -1.7105, -0.9456, -0.2314,  2.2862, -1.0982,  0.6176,  1.9784],\n",
      "        [-0.5686,  1.3279,  0.0929, -1.1565,  0.5139, -0.1223,  0.8788,  2.0444,\n",
      "          0.2803,  0.6417, -0.3809,  0.2828, -0.3895, -2.7013, -1.4182,  0.2743,\n",
      "         -1.0461, -1.5824, -2.0993, -1.3979,  1.3412, -0.4346,  1.5427,  1.2284,\n",
      "         -2.0168, -1.3084,  0.2939,  2.2792,  1.2569, -0.8994,  1.0784, -0.0203],\n",
      "        [ 0.1340,  0.2584, -0.5792, -0.5029,  2.7529,  0.0107,  0.8058,  2.3262,\n",
      "         -1.9017, -0.4165, -1.4422, -0.7689, -0.7657, -1.3836,  0.7729, -0.0596,\n",
      "          0.1377, -0.9144, -1.0304, -2.4873,  2.4421, -0.2146,  1.3113,  2.0114,\n",
      "         -0.5655, -1.5423,  1.9430,  2.1211,  1.2265, -0.4562,  0.4370,  1.1741],\n",
      "        [ 0.5933,  1.3073,  0.5726, -0.0917,  1.6623,  1.3124,  0.8131,  1.4753,\n",
      "         -1.6077,  1.4744, -0.6149, -0.1319,  0.2843, -2.1551, -1.0225,  1.1611,\n",
      "         -0.7732, -1.3496, -0.7587, -1.4566,  1.8770,  0.2448,  0.9532,  0.2902,\n",
      "         -1.4033, -1.9129,  1.0373, -0.1574,  2.0627, -1.1652,  0.8938,  0.7756],\n",
      "        [ 0.6018,  1.0445, -0.5415,  0.5355,  1.4569,  0.5330,  0.2956,  1.5574,\n",
      "         -0.2669, -1.4242,  1.5775,  1.0870, -0.6438, -1.5680, -1.4657,  1.3033,\n",
      "         -0.6602, -0.7102, -1.1306, -1.5142,  1.2747,  0.5494, -0.2278,  1.8629,\n",
      "         -1.8720, -0.3860,  1.0929,  1.4837,  1.2602, -1.6315, -0.4450,  0.6793],\n",
      "        [-0.4931, -0.2156, -1.0300, -1.1251,  2.3141,  0.1844,  0.0278,  1.1525,\n",
      "         -0.3218, -0.2236, -0.9952,  0.4091, -0.8534, -2.1377, -2.0955,  0.4107,\n",
      "         -0.5804, -1.6455, -1.4729, -2.6273,  1.5917, -0.3360,  2.3430,  0.6596,\n",
      "         -1.4888, -1.8436,  1.2947,  2.4997,  1.9382, -0.2631,  0.8981,  0.6717],\n",
      "        [-0.6251,  1.0291, -0.9705, -0.5551,  1.3933,  1.4241,  0.6316,  0.8137,\n",
      "         -0.1443, -0.4631,  0.1315, -0.3589, -0.3534, -1.7653, -0.1728,  0.4081,\n",
      "         -2.4594, -2.0278, -0.9450, -1.9469,  0.8574, -0.0176,  0.7410,  0.8269,\n",
      "         -0.9872, -1.0237,  1.6763,  1.2756,  1.2098, -0.8075,  1.2227,  1.8007]],\n",
      "       grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence, Tuple\n",
    "\n",
    "def get_top_K(model, uid : int, k : int) -> Tuple[ Sequence[int], Sequence[float],  np.ndarray ] :\n",
    "  #returns iids, their (normalised) scores in descending order, and item emebddings for the top k predictions of the given uid.\n",
    "\n",
    "  from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "  from scipy.stats import rankdata\n",
    "  # get scores from model\n",
    "  scores = model.predict(uid)\n",
    "\n",
    "  # map scores into rank 0..1 over the entire item space\n",
    "  scores = minmax_scale(scores)\n",
    "\n",
    "  #compute their ranks  \n",
    "  ranks = rankdata(-scores)\n",
    "  \n",
    "  # get and filter iids, scores and embeddings\n",
    "  rtr_scores = scores[ranks <= k]\n",
    "  rtr_iids = np.argwhere(ranks <= k).flatten()\n",
    "  if hasattr(model, '_net'):\n",
    "    embs = model._net.item_embeddings.weight[rtr_iids]\n",
    "  else:\n",
    "    # not a model that has any embeddings\n",
    "    embs = np.zeros([k,1])\n",
    "\n",
    "  # identify correct ordering using numpy.argsort()\n",
    "  ordering = (-1*rtr_scores).argsort()\n",
    "\n",
    "  #return iids, scores and their embeddings in descending order of score\n",
    "  return rtr_iids[ordering], rtr_scores[ordering], embs[ordering]\n",
    "\n",
    "if BPRMF is not None:\n",
    "  iids, scores, embs = get_top_K(BPRMF, 0, 10)\n",
    "  print(\"Returned iids: %s\" % str(iids))\n",
    "  print(\"Returned scores: %s\" % str(scores))\n",
    "  print(\"Returned embeddings: %s\" % str(embs))\n",
    "else:\n",
    "  print(\"You need to define BPRMF in Task 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVYiQRYaEh61"
   },
   "source": [
    "## Task 3. Evaluation of Non-personalised Models\n",
    "Implement the following four (non-personalised) baselines for ranking books based on their statistics:\n",
    " - Average rating, obtained from ratings_df, `ratings` column\n",
    " - Number of ratings, obtained from books_df (column `ratings_count`)\n",
    " - Number of 5* ratings, obtained from books_df (column `ratings_5`)\n",
    " - Fraction of 5* ratings, calculated from the two sources of evidence above, i.e (columns  `ratings_5` and `ratings_count`).\n",
    "\n",
    "Evaluate these in terms of MRR using the provided test data. You may use the StaticModel class below. \n",
    "\n",
    "Hints: \n",
    " - As in Exercise 2, the order of items returned by predict() is _critical_. You may wish to refer to iid_map.\n",
    " - For all models, you need to ensure that your values are not cast to ints. If you are extracting values from a Pandas series, it is advised to use [.astype(np.float32)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.astype.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:32.561125Z",
     "start_time": "2021-08-06T12:40:32.557334Z"
    },
    "id": "Xop8aPfyFucw"
   },
   "outputs": [],
   "source": [
    "class StaticModel:\n",
    "  \n",
    "  def __init__(self, staticscores):\n",
    "    self.numitems = len(staticscores)\n",
    "    #print(self.numitems)\n",
    "    assert isinstance(staticscores, np.ndarray), \"Expected a numpy array\"\n",
    "    assert staticscores.dtype == np.float32 or staticscores.dtype == np.float64, \"Expected a numpy array of floats\"\n",
    "    self.staticscores = staticscores\n",
    "  \n",
    "  def predict(self, uid):\n",
    "    #this model returns the same scores for each user    \n",
    "    return self.staticscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:33.162431Z",
     "start_time": "2021-08-06T12:40:32.562719Z"
    },
    "id": "6R1q-Zm7FVM9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015052024168984034\n"
     ]
    }
   ],
   "source": [
    "# Add your solution here\n",
    "\n",
    "# Average rating, obtained from ratings_df, ratings column\n",
    "avg_ratings_origin = ratings_df.groupby(['book_id'])['rating'].mean().astype(np.float32)\n",
    "# change the order to be the same as prediction\n",
    "avg_ratings = [avg_ratings_origin[bid] for bid in iid_map.keys()]\n",
    "avg_ratings_model = StaticModel(np.array(avg_ratings))\n",
    "# evaluate the model in terms of MRR\n",
    "print(mrr_score(avg_ratings_model, test_dataset, train=rating_dataset, k=100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:34.332703Z",
     "start_time": "2021-08-06T12:40:33.171034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2396001188245477\n"
     ]
    }
   ],
   "source": [
    "# Number of ratings, obtained from books_df (column ratings_count)\n",
    "num_ratings = [books_df[books_df['book_id'] == bid]['ratings_count'].iloc[0].astype(np.float32) for bid in iid_map.keys()]\n",
    "num_ratings_model = StaticModel(np.array(num_ratings))\n",
    "# evaluate the model in terms of MRR\n",
    "print(mrr_score(num_ratings_model, test_dataset, train=rating_dataset, k=100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:35.481004Z",
     "start_time": "2021-08-06T12:40:34.335703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2409670879930144\n"
     ]
    }
   ],
   "source": [
    "# Number of 5* ratings, obtained from books_df (column ratings_5)\n",
    "num_5_ratings = [books_df[books_df['book_id'] == bid]['ratings_5'].iloc[0].astype(np.float32) for bid in iid_map.keys()]\n",
    "num_5_ratings_model = StaticModel(np.array(num_5_ratings))\n",
    "# evaluate the model in terms of MRR\n",
    "print(mrr_score(num_5_ratings_model, test_dataset, train=rating_dataset, k=100).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.044091Z",
     "start_time": "2021-08-06T12:40:35.482719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03415267465103555\n"
     ]
    }
   ],
   "source": [
    "# Fraction of 5* ratings, calculated from the two sources of evidence above, i.e (columns ratings_5 and ratings_count).\n",
    "frac_5_ratings = np.array(num_5_ratings) / np.array(num_ratings)\n",
    "frac_5_ratings_model = StaticModel(frac_5_ratings)\n",
    "# evaluate the model in terms of MRR\n",
    "print(mrr_score(frac_5_ratings_model, test_dataset, train=rating_dataset, k=100).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZFDmFcdXPFl"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZqRSixpGvfn"
   },
   "source": [
    "## Task 4. Qualiatively Examining Recommendations\n",
    "\n",
    "From now on, we will consider the `BPRMF` model.\n",
    "\n",
    "In Recommender Systems, the ground truth (i.e. our list of books that the user has added to their \"to_read\" shelf) can be very incomplete. For instance, this can be because the user is not aware of the book yet.\n",
    "\n",
    "For this reason, it is important to \"eyeball\" the recommendations, to understand what the system is surfacing, and whether the recommendations make sense. In this way, we understand if the recommendations are reasonable, even if they are for books that the user has not actually read according to the test dataset.\n",
    "\n",
    "First, write a function, which given a uid (int), prints the *title and authors* of:\n",
    " - (a) the books that the user has previously shelved (c.f. `toread_dataset`)\n",
    " - (b) the books that the user will read in the future (c.f. `test_dataset`)\n",
    " - (c) the top 10 books that the user were recommended by `BPRMF` - you can make use of `get_top_K()`.\n",
    "\n",
    "You can use the previously defined `getAuthorTitle()` function in your solution.\n",
    "You will also want to compare books in (c) with those in (a) and (b).\n",
    "\n",
    "Then, we will examine two specific users, namely uid 1805 (u336) and uid 179 (user u1331), to analyse if their recommendations make sense. Refer to the Task 4 quiz questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.052804Z",
     "start_time": "2021-08-06T12:40:36.046201Z"
    },
    "id": "Kg1eFa5GYv5c"
   },
   "outputs": [],
   "source": [
    "# Add your solution here\n",
    "def analyse_user(uid):\n",
    "#      the books that the user has previously shelved (c.f. toread_dataset)\n",
    "    shelved_books = toread_iids[toread_uids == uid].tolist()\n",
    "    print(\"*\"*50)\n",
    "    print(\"The books that user %s has previously shelved:\" % uid)\n",
    "    for book in shelved_books:\n",
    "        print(getAuthorTitle(book))\n",
    "#     the books that the user will read in the future (c.f. test_dataset)\n",
    "    future_books = test_iids[test_uids == uid].tolist()\n",
    "    print(\"*\"*50)\n",
    "    print(\"The books that user %s will read in the future:\" % uid)\n",
    "    for book in future_books:\n",
    "        print(getAuthorTitle(book))\n",
    "#     the top 10 books that the user were recommended by BPRMF\n",
    "    rec_books = get_top_K(BPRMF, uid=uid, k=10)[0].tolist()\n",
    "    print(\"*\"*50)\n",
    "    print(\"The books BPRMF recommended for user %s\" % uid)\n",
    "    for book in rec_books:\n",
    "        print(getAuthorTitle(book))\n",
    "#     compare books recommended with those has been shelved\n",
    "    common_ac = list(set(shelved_books).intersection(set(rec_books)))\n",
    "    print(\"*\"*50)\n",
    "    print(\"Recommended books which have been shelved:\")\n",
    "    for book in common_ac:\n",
    "        print(getAuthorTitle(book))\n",
    "#     compare books recommended with those will be read in the future\n",
    "    common_bc = list(set(future_books).intersection(set(rec_books)))\n",
    "    print(\"*\"*50)\n",
    "    print(\"Recommended books which will be read in the future:\")\n",
    "    for book in common_bc:\n",
    "        print(getAuthorTitle(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.062505Z",
     "start_time": "2021-08-06T12:40:36.054738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05555555555555555"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BPR_rr[1805]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.104654Z",
     "start_time": "2021-08-06T12:40:36.063927Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "The books that user 1805 has previously shelved:\n",
      "Stieg Larsson, Reg Keeland / The Girl Who Kicked the Hornet's Nest (Millennium, #3)\n",
      "Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
      "Dennis Lehane / Shutter Island\n",
      "Suzanne Collins / Catching Fire (The Hunger Games, #2)\n",
      "Paula Hawkins / The Girl on the Train\n",
      "Robert Ludlum / The Bourne Supremacy (Jason Bourne, #2)\n",
      "John Grisham / The Client\n",
      "Thomas Harris / The Silence of the Lambs  (Hannibal Lecter, #2)\n",
      "Daphne du Maurier, Sally Beauman / Rebecca\n",
      "Robert Ludlum / The Bourne Identity (Jason Bourne, #1)\n",
      "Robert Galbraith, J.K. Rowling / The Cuckoo's Calling (Cormoran Strike, #1)\n",
      "Stephen King / Misery\n",
      "Michael Crichton / Jurassic Park (Jurassic Park, #1)\n",
      "Robert Ludlum / The Bourne Ultimatum (Jason Bourne, #3)\n",
      "Stephen King, Bernie Wrightson / The Stand\n",
      "Michael Crichton / The Andromeda Strain\n",
      "Thomas Harris / Red Dragon (Hannibal Lecter, #1)\n",
      "Lee Child / Die Trying (Jack Reacher, #2)\n",
      "Lee Child / Worth Dying For (Jack Reacher, #15)\n",
      "Lee Child / Tripwire  (Jack Reacher, #3)\n",
      "Michael Crichton / Congo\n",
      "Lee Child, Dick Hill / Without Fail (Jack Reacher, #6)\n",
      "Michael Crichton / The Lost World (Jurassic Park, #2)\n",
      "Janet Evanovich / One for the Money (Stephanie Plum, #1)\n",
      "Tom Clancy / Patriot Games (Jack Ryan Universe, #2)\n",
      "Lee Child / Running Blind (Jack Reacher, #4)\n",
      "Ken Follett / Eye of the Needle\n",
      "Michael Crichton / State of Fear\n",
      "Scott Turow / Presumed Innocent\n",
      "Harlan Coben / Tell No One\n",
      "**************************************************\n",
      "The books that user 1805 will read in the future:\n",
      "John Grisham / The Pelican Brief\n",
      "Stieg Larsson, Reg Keeland / The Girl Who Played with Fire (Millennium, #2)\n",
      "Gillian Flynn / Gone Girl\n",
      "Tom Clancy / The Hunt for Red October (Jack Ryan Universe, #4)\n",
      "Chuck Palahniuk / Fight Club\n",
      "Umberto Eco, William Weaver, Seán Barrett / The Name of the Rose\n",
      "John Grisham / The Runaway Jury\n",
      "Thomas Harris / Hannibal (Hannibal Lecter, #3)\n",
      "Lee Child / The Affair (Jack Reacher, #16)\n",
      "John Grisham / The Firm (Penguin Readers, Level 5)\n",
      "Lee Child / Killing Floor (Jack Reacher, #1)\n",
      "John Grisham / A Time to Kill\n",
      "Stephen King / The Shining (The Shining #1)\n",
      "Michael Crichton / Timeline\n",
      "Michael Crichton / Prey\n",
      "Jeffery Deaver / The Bone Collector (Lincoln Rhyme, #1)\n",
      "**************************************************\n",
      "The books BPRMF recommended for user 1805\n",
      "Suzanne Collins / The Hunger Games (The Hunger Games, #1)\n",
      "Dan Brown / The Da Vinci Code (Robert Langdon, #2)\n",
      "Dan Brown / The Lost Symbol (Robert Langdon, #3)\n",
      "Michael Crichton / Disclosure\n",
      "George R.R. Martin / A Clash of Kings  (A Song of Ice and Fire, #2)\n",
      "Dan Brown / Angels & Demons  (Robert Langdon, #1)\n",
      "John Grisham / The Broker\n",
      "Khaled Hosseini / The Kite Runner\n",
      "George R.R. Martin / A Game of Thrones (A Song of Ice and Fire, #1)\n",
      "Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
      "**************************************************\n",
      "Recommended books which have been shelved:\n",
      "Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
      "**************************************************\n",
      "Recommended books which will be read in the future:\n"
     ]
    }
   ],
   "source": [
    "# uid 1805 (u336) and uid 179 (user u1331)\n",
    "analyse_user(1805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.146717Z",
     "start_time": "2021-08-06T12:40:36.106313Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "The books that user 179 has previously shelved:\n",
      "Dan Brown / Angels & Demons  (Robert Langdon, #1)\n",
      "Suzanne Collins / The Hunger Games (The Hunger Games, #1)\n",
      "Antoine de Saint-Exupéry, Richard Howard, Dom Marcos Barbosa, Melina Karakosta / The Little Prince\n",
      "Truman Capote / Breakfast at Tiffany's\n",
      "Dan Brown / The Da Vinci Code (Robert Langdon, #2)\n",
      "Laura Ingalls Wilder, Garth Williams / Little House on the Prairie (Little House, #2)\n",
      "Milan Kundera, Michael Henry Heim / The Unbearable Lightness of Being\n",
      "Suzanne Collins / Catching Fire (The Hunger Games, #2)\n",
      "John Grisham / The Client\n",
      "J.R.R. Tolkien / The Lord of the Rings (The Lord of the Rings, #1-3)\n",
      "J.R.R. Tolkien / The Hobbit\n",
      "Margaret Mitchell / Gone with the Wind\n",
      "Neil Gaiman / Stardust\n",
      "Laura Ingalls Wilder, Garth Williams / Little House in the Big Woods (Little House, #1)\n",
      "Pearl S. Buck / The Good Earth (House of Earth, #1)\n",
      "Dan Brown / Digital Fortress\n",
      "Daniel Keyes / Flowers for Algernon\n",
      "Neil Gaiman / Coraline\n",
      "Dan Brown / Deception Point\n",
      "John Grisham / The Broker\n",
      "John Grisham / The Brethren\n",
      "Agatha Christie / Murder on the Orient Express (Hercule Poirot, #10)\n",
      "John Grisham / The King of Torts\n",
      "Flora Rheta Schreiber / Sybil: The Classic True Story of a Woman Possessed by Sixteen Personalities\n",
      "John Grisham / The Street Lawyer\n",
      "John Grisham / The Partner\n",
      "Anthony Bourdain / Kitchen Confidential: Adventures in the Culinary Underbelly\n",
      "John Grisham / The Rainmaker\n",
      "Astrid Lindgren, Lauren Child, Florence Lamborn, Nancy Seligsohn / Pippi Longstocking\n",
      "Sidney Sheldon / If Tomorrow Comes (Tracy Whitney Series, #1)\n",
      "**************************************************\n",
      "The books that user 179 will read in the future:\n",
      "Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
      "J.K. Rowling, Mary GrandPré / Harry Potter and the Deathly Hallows (Harry Potter, #7)\n",
      "Agatha Christie, Ροζίτα Σώκου / The Mysterious Affair at Styles (Hercule Poirot, #1)\n",
      "John Grisham / The Testament\n",
      "John Grisham / The Innocent Man: Murder and Injustice in a Small Town\n",
      "John Grisham / A Painted House\n",
      "Agatha Christie / The Man in the Brown Suit\n",
      "John Grisham / The Summons\n",
      "Paulo Coelho / The Witch Of Portobello\n",
      "Laura Ingalls Wilder, Garth Williams / Little Town on the Prairie  (Little House, #7)\n",
      "**************************************************\n",
      "The books BPRMF recommended for user 179\n",
      "John Grisham / The Partner\n",
      "John Grisham / The Pelican Brief\n",
      "John Grisham / The Client\n",
      "John Grisham / The Brethren\n",
      "John Grisham / The Street Lawyer\n",
      "John Grisham / The Broker\n",
      "John Grisham / The Rainmaker\n",
      "John Grisham / The King of Torts\n",
      "J.K. Rowling, Mary GrandPré / Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\n",
      "John Grisham / The Runaway Jury\n",
      "**************************************************\n",
      "Recommended books which have been shelved:\n",
      "John Grisham / The Brethren\n",
      "John Grisham / The Broker\n",
      "John Grisham / The Street Lawyer\n",
      "John Grisham / The Partner\n",
      "John Grisham / The King of Torts\n",
      "John Grisham / The Client\n",
      "John Grisham / The Rainmaker\n",
      "**************************************************\n",
      "Recommended books which will be read in the future:\n"
     ]
    }
   ],
   "source": [
    "analyse_user(179)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES_zHeCkNBeC"
   },
   "source": [
    "# Part-C. Diversity of Recommendations\n",
    "\n",
    "This part of the exercise is concerned with diversification, as covered in Lecture 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvBep-ROHWSX"
   },
   "source": [
    "## Task 5. Measuring Intra-List Diversity\n",
    "\n",
    "\n",
    "For the BPR implicit factorisation model, implement the Intra-list diversity measure (see Lecture 11) of the top 5 scored items based on their item embeddings in the `BPRMF` model. \n",
    "\n",
    "Implement your ILD as a function with the specification:\n",
    "```python\n",
    "def measure_ild(top_books : Sequence[int], K : int=5) -> float\n",
    "```\n",
    "where:\n",
    " - `top_books` is a list or a Numpy array of iids that have been returned for a particular user. For instance, it can be obtained from `get_top_K()`.\n",
    " - `K` is the number of top-ranked items to consider from `top_books`. \n",
    " - Your implementation should use the item emebddings stored in the `BPRMF` model.\n",
    "\n",
    "Calculate the ILD (with k=5). Using your code for Task 4, identify the books previously shelved and recommended for the specific users requested in the quiz, and use these to analyse the recommendations.\n",
    "\n",
    "Hints:\n",
    " - As can be seen in `get_top_K()`, item embeddings can be obtained from `BPRMF._net.item_embeddings.weight[iid]`.\n",
    " - For obtaining the cosine similarity of PyTorch tensors, use `nn.functional.cosine_similarity(, , axis=0)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.152845Z",
     "start_time": "2021-08-06T12:40:36.148355Z"
    },
    "id": "4n2vBwcnYuM4"
   },
   "outputs": [],
   "source": [
    "# Add your solution here\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def measure_ild(top_books : Sequence[int], K : int=5) -> float:\n",
    "#     get embeddings for each iid\n",
    "    embeddings = [BPRMF._net.item_embeddings.weight[iid] for iid in top_books]\n",
    "    distances = []\n",
    "    for i in range(K):\n",
    "        for j in range(i + 1, K):\n",
    "#             calculate cosine similarity between each pair of i and j\n",
    "            sim = nn.functional.cosine_similarity(embeddings[i], embeddings[j], axis=0)\n",
    "#             calculate pairwise distance\n",
    "            distances.append(1 - sim.detach().numpy())\n",
    "    ILD = (2 / (K * (K - 1))) * np.sum(distances)\n",
    "    return ILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.162583Z",
     "start_time": "2021-08-06T12:40:36.154604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7484900385141373"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_ild(get_top_K(BPRMF, uid=1805, k=5)[0].tolist(), K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.174027Z",
     "start_time": "2021-08-06T12:40:36.164387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suzanne Collins / The Hunger Games (The Hunger Games, #1)\n",
      "Dan Brown / The Da Vinci Code (Robert Langdon, #2)\n",
      "Dan Brown / The Lost Symbol (Robert Langdon, #3)\n",
      "Michael Crichton / Disclosure\n",
      "George R.R. Martin / A Clash of Kings  (A Song of Ice and Fire, #2)\n"
     ]
    }
   ],
   "source": [
    "for book in get_top_K(BPRMF, uid=1805, k=5)[0].tolist():\n",
    "    print(getAuthorTitle(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.183478Z",
     "start_time": "2021-08-06T12:40:36.175460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27872802019119264"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_ild(get_top_K(BPRMF, uid=179, k=5)[0].tolist(), K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.194506Z",
     "start_time": "2021-08-06T12:40:36.184875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Grisham / The Partner\n",
      "John Grisham / The Pelican Brief\n",
      "John Grisham / The Client\n",
      "John Grisham / The Brethren\n",
      "John Grisham / The Street Lawyer\n"
     ]
    }
   ],
   "source": [
    "for book in get_top_K(BPRMF, uid=179, k=5)[0].tolist():\n",
    "    print(getAuthorTitle(book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qwrP1jUpARF"
   },
   "source": [
    "## Task 6. Implement MMR Diversification \n",
    "\n",
    "Develop an Maximal Marginal Relevance (M**M**R) diversification technique, to re-rank the top-ranked recommendations for a given user.\n",
    "\n",
    "Your function should adhere to the specification as follows:\n",
    "```python\n",
    "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
    "```\n",
    "\n",
    "where iids is a list of iids, scores are their corresponding scores (in descending order), embs is their embeddings, and alpha controls the diversification tradeoff. The function returns a re-ordering of iids. As in previous Exercises, type hints are provided for clarity; a Sequence can be a list or numpy array. \n",
    "\n",
    "Hints:\n",
    " - As above, for obtaining the cosine similarity of PyTorch tensors, use nn.functional.cosine_similarity(, , axis=0).\n",
    "\n",
    "To use your `mmr()` function, provide it with the outputs of `get_top_K()`. For example, to obtain an MMR reordering of the top 10 predictions of uid 0, we can run:\n",
    "```\n",
    "mmr( *get_top_K(bprmodel, 0, 10), 0.5)\n",
    "```\n",
    "\n",
    "Thereafter, we provide test cases for your MMR implementation, which you  should report in the quiz. We also ask for the ILD values before and after the application of MMR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.203904Z",
     "start_time": "2021-08-06T12:40:36.196408Z"
    },
    "id": "1VkEMfRvIhKV"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
    "\n",
    "    assert len(iids) == len(scores)\n",
    "    assert len(iids) == embs.shape[0]\n",
    "    assert len(embs.size()) == 2\n",
    "    \n",
    "    iteration = len(iids)\n",
    "    iids = iids.tolist() if not isinstance(iids, list) else iids\n",
    "    scores = scores.tolist() if not isinstance(scores, list) else scores\n",
    "    rtr_iids = []\n",
    "    rtr_embs = []\n",
    "\n",
    "#     the overall iterations\n",
    "    for i in range(iteration):\n",
    "#      save all the results for elements in R in one iteration to calculate MMR \n",
    "        mmr_list = []\n",
    "#      iterate in R (iids)\n",
    "        for itemi in range(len(iids)):\n",
    "#         a list of similarity for item i and all elements in S\n",
    "            cos_sims = []\n",
    "#             compare cosine similarity between item i in R and item j in S\n",
    "            for itemj in range(len(rtr_iids)):\n",
    "                cos_sim = nn.functional.cosine_similarity(embs[itemi], rtr_embs[itemj], axis=0)\n",
    "                cos_sims.append(cos_sim.detach().numpy())\n",
    "#          compute for each item currently in R\n",
    "            mmr_itemi = (alpha * scores[itemi] - (1 - alpha) * max(cos_sims)) if len(cos_sims) > 0 else (alpha * scores[itemi])\n",
    "            mmr_list.append(mmr_itemi)\n",
    "#      get the index of max value, that is the index that will be removed from R and added to S\n",
    "        idx = np.argmax(mmr_list)\n",
    "        rtr_iids.append(iids[idx])\n",
    "        rtr_embs.append(embs[idx])\n",
    "        iids.remove(iids[idx])\n",
    "        scores.remove(scores[idx])\n",
    "        embs = embs[torch.arange(embs.size(0)) != idx]\n",
    "        \n",
    "  #input your solution here returns a re-ordering of iids, such that the first ranked item is first in the list\n",
    "\n",
    "    return rtr_iids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.217692Z",
     "start_time": "2021-08-06T12:40:36.205458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testcase 0 : 1\n",
      "Testcase 1 : 3\n",
      "Testcase 2 : 2\n",
      "Testcase 3 : 4\n",
      "Testcase 4 : 3\n"
     ]
    }
   ],
   "source": [
    "def run_MMR_testcases(mmrfn):\n",
    "  example_embeddings1 = torch.tensor([[1.0,1.0],[1.0,1.0],[0,1.0],[0.1, 1.0]])\n",
    "  example_embeddings2 = torch.tensor([[1.0,1.0],[1.0,1.0],[0.02,1.0],[0.01,1.0]])\n",
    "  print(\"Testcase 0 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[0] )\n",
    "  print(\"Testcase 1 : %s\" % mmrfn([1,2,3,4], [0.5, 0.5, 0.5, 0.5],  example_embeddings1, 0.5)[1] )\n",
    "  print(\"Testcase 2 : %s\" % mmrfn([1,2,3,4], [4, 3, 2, 1],  example_embeddings1, 1)[1] )\n",
    "  print(\"Testcase 3 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.001)[1] )\n",
    "  print(\"Testcase 4 : %s\" % mmrfn([1,2,3,4], [0.99, 0.98, 0.97, 0.001],  example_embeddings2, 0.5)[1] )\n",
    "\n",
    "run_MMR_testcases(mmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfm9mCWZmBPQ"
   },
   "source": [
    "Now we can analyse the impact of our MMR implementation. Let's consider again uid 179 (user u1331). \n",
    "\n",
    "Apply MMR on the top 10 results obtained from the BPRMF model using `get_top_K()`, with an alpha value of 0.5. The following code should help:\n",
    "```python\n",
    "mmr( *get_top_K(bprmodel, 179, 10), 0.5)\n",
    "```\n",
    "\n",
    "Finally, anayse the returned books. Calculate the ILD (with `k=5`), and examine the authors and titles (using `getAuthorTitle()`). \n",
    "\n",
    "Now answer the questions in Task 6 of the Moodle quiz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-06T12:40:36.258754Z",
     "start_time": "2021-08-06T12:40:36.219297Z"
    },
    "id": "1wM7m8pOmCnM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5566441953182221\n",
      "89\n",
      "John Grisham / The Partner\n",
      "9\n",
      "J.K. Rowling, Mary GrandPré / Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\n",
      "88\n",
      "John Grisham / The Street Lawyer\n",
      "391\n",
      "John Grisham / The Pelican Brief\n",
      "906\n",
      "John Grisham / The Brethren\n",
      "62\n",
      "John Grisham / The Rainmaker\n",
      "934\n",
      "John Grisham / The Runaway Jury\n",
      "86\n",
      "John Grisham / The Broker\n",
      "92\n",
      "John Grisham / The Client\n",
      "91\n",
      "John Grisham / The King of Torts\n"
     ]
    }
   ],
   "source": [
    "#add your solution here\n",
    "print(measure_ild(mmr( *get_top_K(BPRMF, 179, 10), 0.5), K=5))\n",
    "for book in mmr( *get_top_K(BPRMF, 179, 10), 0.5):\n",
    "    print(book)\n",
    "    print(getAuthorTitle(book))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49QoGVphnegD"
   },
   "source": [
    "# Task 7\n",
    "\n",
    "This task is not a practical task - instead there are questions that tests your understanding of some related content of the course in the quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8N-fEdkMKLD"
   },
   "source": [
    "# End of Exercise\n",
    "\n",
    "As part of your submission, you should complete the Exercise 3 quiz on Moodle.\n",
    "You will need to upload your notebook, complete with the **results** of executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:26:54.519694Z",
     "start_time": "2021-08-19T09:26:54.512089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38807526, 0.38807526, 0.31046021, 0.23284516, 0.31046021,\n",
       "       0.38807526, 0.38807526, 0.31046021, 0.23284516])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit centring\n",
    "a = np.array([5,5,4,3,4,5,5,4,3])\n",
    "\n",
    "a / np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:30:28.856080Z",
     "start_time": "2021-08-19T09:30:28.850910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.77777778,  0.77777778, -0.22222222, -1.22222222, -0.22222222,\n",
       "        0.77777778,  0.77777778, -0.22222222, -1.22222222])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean centring\n",
    "b = np.array([5,5,4,3,4,5,5,4,3])\n",
    "\n",
    "b - b.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-19T09:31:42.782927Z",
     "start_time": "2021-08-19T09:31:42.776209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98994949,  0.98994949, -0.28284271, -1.55563492, -0.28284271,\n",
       "        0.98994949,  0.98994949, -0.28284271, -1.55563492])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Z score normalisation\n",
    "\n",
    "(b - b.mean()) / np.std(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RecSys M Ex3 TEMPLATE.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
